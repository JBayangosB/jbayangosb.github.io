<!doctype html><html lang=en><head><title>Train Flux Lora · Jayden Bayangos-Brandt
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Jayden Bayangos-Brandt"><meta name=description content="Something"><meta name=keywords content="blog,developer,personal"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="Train Flux Lora"><meta name=twitter:description content="Something"><meta property="og:url" content="https://jbayangosb.github.io/posts/train-flux-lora/"><meta property="og:site_name" content="Jayden Bayangos-Brandt"><meta property="og:title" content="Train Flux Lora"><meta property="og:description" content="Something"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-18T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-18T00:00:00+00:00"><link rel=canonical href=https://jbayangosb.github.io/posts/train-flux-lora/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.07092c1350ffd254998dc43a44ae96e617d14af4df4602626878df89189c5e1a.css integrity="sha256-BwksE1D/0lSZjcQ6RK6W5hfRSvTfRgJiaHjfiRicXho=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/custom.min.c0697e8d94dbbf3f3b49ac23a155539cc56f0bf8a582f3314e00ca8a3354c5fc.css integrity="sha256-wGl+jZTbvz87SawjoVVTnMVvC/ilgvMxTgDKijNUxfw=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://jbayangosb.github.io/>Jayden Bayangos-Brandt
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://jbayangosb.github.io/posts/train-flux-lora/>Train Flux Lora</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2024-09-18T00:00:00Z>September 18, 2024
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
4-minute read</span></div><div class=authors><i class="fa-solid fa-user" aria-hidden=true></i>
<a href=/authors/jayden-bayangos-brandt/>Jayden Bayangos-Brandt</a></div></div></header><div class=post-content><h1 id=how-to-train-a-flux-lora-to-make-pictures-of-you>How to train a Flux LORA to make pictures of you!
<a class=heading-link href=#how-to-train-a-flux-lora-to-make-pictures-of-you><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>I had a lot of difficulty trying to figure out how to train a LORA for flux on my system.</p><p>(4070 with 12GB VRAM)</p><p>So after a bunch of trial and error I compiled this mini tutorial with as much guidance as I needed.</p><p>Hopefully this can help someone else out in the future!</p><p>There is probably a good chance this is out-of-date and there are better ways to do this when this is live, oh well!</p><h2 id=setup>Setup
<a class=heading-link href=#setup><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><h3 id=environment-setup>Environment setup
<a class=heading-link href=#environment-setup><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Clone the following git repo with the command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone -b sd3 https://github.com/kohya-ss/sd-scripts.git
</span></span></code></pre></div><p>or if that doesn&rsquo;t end up working due to updates. This is the commit I was checked out on when I succeeded.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git checkout 2e89cd2cc634c27add7a04c21fcb6d0e16716a2b
</span></span></code></pre></div><p>Navigate via the terminal to the newly cloned <code>sd-scripts</code> folder.</p><p>Call the commands in sequence:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python -m venv venv
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>.<span class=se>\v</span>env<span class=se>\S</span>cripts<span class=se>\a</span>ctivate
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip3 install <span class=nv>torch</span><span class=o>==</span>2.4.0 <span class=nv>torchvision</span><span class=o>==</span>0.19.0 --index-url https://download.pytorch.org/whl/cu124
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install --upgrade -r requirements.txt
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>accelerate config
</span></span></code></pre></div><p>The above command will display a series of options, you should select these answers:</p><pre tabindex=0><code>    - This machine
    - No distributed training
    - NO
    - NO
    - NO
    - all
    - fp16
</code></pre><p>By default inside <code>sd-scripts</code> there will be a file called <code>dataset.toml</code>, this is where you need configure the following settings:</p><ul><li>The resolution of the images in your dataset</li><li>The directory of the images</li><li>The number of repeats</li><li>The token you want to use in prompts to trigger the LORA.</li></ul><p>Here is what a toml might look like which you can use to update yours:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-toml data-lang=toml><span class=line><span class=cl><span class=p>[</span><span class=nx>general</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nx>shuffle_caption</span> <span class=p>=</span> <span class=kc>false</span>
</span></span><span class=line><span class=cl><span class=nx>caption_extension</span> <span class=p>=</span> <span class=s1>&#39;.txt&#39;</span>
</span></span><span class=line><span class=cl><span class=nx>keep_tokens</span> <span class=p>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c># This is a DreamBooth-style dataset</span>
</span></span><span class=line><span class=cl><span class=p>[[</span><span class=nx>datasets</span><span class=p>]]</span>
</span></span><span class=line><span class=cl><span class=nx>resolution</span> <span class=p>=</span> <span class=mi>512</span> <span class=c># Update this with your resolution, aim for either 512 or 1024.</span>
</span></span><span class=line><span class=cl><span class=nx>batch_size</span> <span class=p>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=nx>keep_tokens</span> <span class=p>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=p>[[</span><span class=nx>datasets</span><span class=p>.</span><span class=nx>subsets</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>  <span class=nx>image_dir</span> <span class=p>=</span> <span class=s1>&#39;C:\dataset\images_to_train_on&#39;</span> <span class=c># Update this with your actual directory. </span>
</span></span><span class=line><span class=cl>  <span class=nx>class_tokens</span> <span class=p>=</span> <span class=s1>&#39;image name&#39;</span> <span class=c># Update this with what you want to trigger the LORA with in your prompts.</span>
</span></span><span class=line><span class=cl>  <span class=nx>num_repeats</span> <span class=p>=</span> <span class=mi>1</span>
</span></span></code></pre></div><h3 id=dataset-grooming>Dataset grooming
<a class=heading-link href=#dataset-grooming><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>It is ideal that you have at least 10 photos of whatever it is your trying to train a LORA for.</p><p>The photos should show the target with a variety of different backgrounds.</p><p>It is also a good idea to have captions for each image and they must be named in reference to the image file. Such as: <code>image_1.jpg</code> to <code>image_1.txt</code> (Tthe <code>.txt</code> file being the caption.)</p><p>You can use whatever way you desire to produce your captions, its entirely up to you and has different impacts on training.</p><ul><li>You can use existing tools like JoyCaption</li><li>Just providing the token as the caption alone<ul><li>Your training a lora for <code>dogs</code>, <code>dogs</code> is the prompt, you can caption all images with just <code>dogs</code> if you like.</li></ul></li></ul><h2 id=let-the-training-begin>Let the training begin!
<a class=heading-link href=#let-the-training-begin><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>With a terminal set to the <code>sd-scripts</code> directory you can now call the following command to start it off, but you need to make sure you update parameters within the command to suit your settings.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>accelerate launch --mixed_precision bf16 --num_cpu_threads_per_process <span class=m>1</span> flux_train_network.py --pretrained_model_name_or_path C://Change//Path//To//flux1-dev.safetensors --clip_l C://Change//Path//To//clip_l.safetensors --t5xxl C://Change//Path//To//t5xxl_fp8_e4m3fn.safetensors --ae C://Change//Path//To//ae.safetensors --cache_latents_to_disk --save_model_as safetensors --sdpa --persistent_data_loader_workers --max_data_loader_n_workers <span class=m>2</span> --seed <span class=m>42</span> --gradient_checkpointing --mixed_precision bf16 --save_precision bf16 --network_module networks.lora_flux --network_dim <span class=m>16</span> --network_alpha <span class=m>16</span> --optimizer_type adafactor --learning_rate 1e-3 --network_train_unet_only --cache_text_encoder_outputs --cache_text_encoder_outputs_to_disk --fp8_base --highvram --max_train_epochs <span class=m>8</span> --save_every_n_epochs <span class=m>2</span> --dataset_config dataset.toml --output_dir C://Change//Path//To//Lora_Save_Directory --output_name lora_name --timestep_sampling sigmoid --model_prediction_type raw --guidance_scale 1.0 --loss_type l2 --optimizer_type adafactor --optimizer_args <span class=s2>&#34;relative_step=False&#34;</span> <span class=s2>&#34;scale_parameter=False&#34;</span> <span class=s2>&#34;warmup_init=False&#34;</span> --split_mode --network_args <span class=s2>&#34;train_blocks=single&#34;</span>
</span></span></code></pre></div><p>In the above block you will need to provide paths to:</p><ul><li><code>flux.dev.safetensors</code></li><li><code>clip_l.safetensors</code></li><li><code>t5xxl_fp8_e4m3fn.safetensors</code></li><li><code>ae.safetensors</code></li><li>Where you want to save your LORA</li></ul><p>The output name of your LORA.</p><p>Once you have executed the above command with your personal parameters updated, you will have kicked off the training process, this can vary greatly in time depending on the settings you selected.</p><p>Dataset count and number of repeats specificed in the <code>.toml</code> file will have the greatest impacts.</p><h2 id=training-done>Training done.
<a class=heading-link href=#training-done><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Congratulations! Assuming all went to plan, you will find a file named <code>lora-name.safetensors</code> inside the directory you specified above. Now you should be able to apply that LORA using the tool of your choice (E.g. ComfyUI).</p><p>I saw decent results with a dataset of 20 diverse images with <code>num_repeats</code> set to 5. This was with caption files that only contained the name of the LORA. In the future I look forward to messing with the parameters further to find the sweet spot.</p><p>I hope this article was helpful!</p><h3 id=sources>Sources
<a class=heading-link href=#sources><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p><a href=https://www.reddit.com/r/StableDiffusion/comments/1eyr9yx/flux_local_lora_training_in_16gb_vram_quick_guide/ class=external-link target=_blank rel=noopener>Reddit - Flux Local LoRA Training in 16GB VRAM (quick guide in my comments)</a></p><p><a href=https://www.reddit.com/r/StableDiffusion/comments/1ezd23b/i_will_train_a_flux_lora_for_you_for_free_3/ class=external-link target=_blank rel=noopener>Reddit - I will train a Flux LORA for you, for free</a></p></div><footer></footer></article></section></div><footer class=footer><section class=container>©
2025
Jayden Bayangos-Brandt
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>